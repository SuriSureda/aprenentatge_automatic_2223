{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FaadnhbpCcsh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly7lrx-gCuLy"
      },
      "source": [
        "# Xarxes convolucionals\n",
        "\n",
        "L'objectiu d'avui és la creació d'una xarxa convolucional que obtengui com a mínim igual resultat que la xarxa completament connectada implementada la setmana anterior però amb menys paràmetres. Per poder realitzar comparacions directes emprarem el mateix conjunt de dades.\n",
        "\n",
        "Com objectius secundaris tenim:\n",
        "\n",
        "1. Aprenentatge de noves estratègies per evitar `overfitting`.\n",
        "2. Us d'un nou optimitzador.\n",
        "3. Visualització dels resultats dels filtres convolucionals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PwSoPhjXCvV9"
      },
      "outputs": [],
      "source": [
        "etiquetes = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 100\n",
        "\n",
        "# Definim una seqüència (composició) de transformacions \n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # mitjana, desviacio tipica\n",
        "    ])\n",
        "\n",
        "# Descarregam un dataset ja integrat en la llibreria Pytorch\n",
        "train = datasets.FashionMNIST('../data', train=True, download=True, transform=transform)\n",
        "test = datasets.FashionMNIST('../data', train=False, transform=transform)\n",
        "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
        "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8i4Mg8KuD3r"
      },
      "source": [
        "## Definició de la xarxa\n",
        "\n",
        "### Feina a fer\n",
        "\n",
        "1. Definir la primera xarxa convolucional. A continuació teniu una llista de les capes que podeu emprar:\n",
        "\n",
        "\n",
        "- `Conv2d`: Capa convolucional en 2 dimensions. Com a paràmetres principals trobarem:\n",
        "\n",
        "  - in_channels: canals d'entrada.\n",
        "  - out_channels : canals de sortida (nombre de filtres).\n",
        "  - kernel_size: mida del filtre.\n",
        "  - stride: desplaçament del filtre. Típicament pren per valor 1.\n",
        "  - padding: ampliació de la imatge per evitar pèrdua de dimensionalitat.\n",
        "\n",
        "- `MaxPool2d`: Capa de max pooling. Aquesta capa no té paràmetres entrenables. Però si:\n",
        "\n",
        "  - kernel_size: Mida del filtre del qual es seleccionarà el màxim.\n",
        "  - stride: desplaçament del filtre.\n",
        "\n",
        "- `Dropout`: Dropout és un mètode de regularització (evitar `overfitting`) que aproxima l'entrenament d'un gran nombre de xarxes neuronals amb diferents arquitectures en paral·lel. Durant l'entrenament, una part de les sortides de la capa s'ignoren aleatòriament o s'abandonen. Això té l'efecte de fer que la capa sembli i es tracti com una capa amb un nombre diferent de nodes i connectivitat a la capa anterior. En efecte, cada actualització d'una capa durant l'entrenament es realitza amb una vista diferent de la capa configurada. Hem d'especificar quines capes tenen `dropout` de manera individual. Té un únic paràmetre amb valor per defecte $p=0.5$ Els valors típics d'aquest paràmetre varien entre $0.5$ i $0.8$.\n",
        "\n",
        "\n",
        "- `Linear`\n",
        "\n",
        "- `ReLU`\n",
        "\n",
        "\n",
        "2. Per posibilitar la visualització de les imatges passades per les capes convolucionals farem que funció `forward`tengui diverses sortides (diferents valors de `return`) un per cadda capa convolucional de la xarxa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IQvdRDtTHdRy"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.max = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=5, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.linear1 = nn.Linear(245, 100) \n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.linear2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        # x = self.relu(x)\n",
        "        x = self.max(x)\n",
        "        output_1 = x\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        # x = self.relu(x)\n",
        "        x = self.max(x)\n",
        "        output_2 = x\n",
        "\n",
        "        x = torch.flatten(x)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        output_result = F.log_softmax(x, dim=1)\n",
        "        return output_result, output_1, output_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6ISOL_hCk7g"
      },
      "source": [
        "## Entrenament\n",
        "\n",
        "Això no varia massa de la setmana anterior\n",
        "\n",
        "### Feina a fer\n",
        "\n",
        "1. Modificar la sortida de la xarxa, ara retorna diversos valors, encara que aquí només us interessa un."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h9OLtpPzClch"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    loss_v = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target, reduction='sum') \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0 and verbose:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
        "        loss_v += loss.item()\n",
        "\n",
        "    loss_v /= len(train_loader.dataset)\n",
        "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
        " \n",
        "    return loss_v\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum') \n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        " \n",
        "  \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBGKL43vsUnD"
      },
      "source": [
        "A continuació definim els paràmetres d'entrenament i el bucle principal:\n",
        "\n",
        "### Adam\n",
        "\n",
        "Aquesta setmana introduirem un nou algorisme d'optimització anomenat `Adam`. Fins ara hem emprat el descens del gradient (`SGD`). \n",
        "\n",
        "`Adam()` és un algorisme d'optimització amplament emprat, tal com el descens del gradient, és iteratiu. A la literatura trobam arguments que indiquen que, tot i que Adam convergeix més ràpidament, SGD  generalitza millor que Adam i, per tant, resulta en un rendiment final millor. \n",
        "\n",
        "[Més info](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)\n",
        "\n",
        "\n",
        "### Feina a fer:\n",
        "1. Mostrar el nombre de paràmetres de la xarxa (també a la xarxa de la setmana passada)\n",
        "```\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "```\n",
        "2. Dibuixar els gràfics de la funció de pèrdua amb les dues funcions d'optimització que coneixem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cNIBWqAwsVSb"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [10, 1, 3, 3], expected input[1, 64, 28, 28] to have 1 channels, but got 64 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Bucle d'entrenament\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[0;32m---> 23\u001b[0m     train_l[epoch] \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, epoch)\n\u001b[1;32m     24\u001b[0m     test_l[epoch]  \u001b[39m=\u001b[39m test(model, device, test_loader)\n",
            "Cell \u001b[0;32mIn [4], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval, verbose)\u001b[0m\n\u001b[1;32m      9\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, target, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [3], line 18\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(x)\n\u001b[0;32m---> 18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax(x)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [10, 1, 3, 3], expected input[1, 64, 28, 28] to have 1 channels, but got 64 channels instead"
          ]
        }
      ],
      "source": [
        "use_cuda = False\n",
        "torch.manual_seed(33)\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "epochs = 15\n",
        "lr =0.00001\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Guardam el valor de peèrdua mig de cada iteració (època)\n",
        "train_l = np.zeros((epochs))\n",
        "test_l = np.zeros((epochs))\n",
        "\n",
        "# Bucle d'entrenament\n",
        "for epoch in range(0, epochs):\n",
        "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
        "    test_l[epoch]  = test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjeMWK8cJkqN"
      },
      "source": [
        "## Resultats\n",
        "\n",
        "Aquí visualitzarem els resultats d'aprenentatge de la xarxa. \n",
        "\n",
        "### Feina a fer:\n",
        "\n",
        "1. Fer una predicció del primer _batch_ del conjunt de _test_.\n",
        "2. Visualitzar una imatge del _batch_ i posar la predicció i el groun truth com a títol de la imatge.\n",
        "3. Visualitzar el resultat de la mateixa imatge passada per tots els filtres de cada convolució de la vostra xarxa.\n",
        "4. **Extra**: Fer la matriu de confusió de les 10 classes per poder entendre el que no estau fent bé (la xarxa no està fent bé).\n",
        "\n",
        "A tenir en compte:\n",
        "\n",
        "#### Subplots\n",
        "\n",
        "Per fer graelles d'imatges podeu empar la funció `subplots`. Més [informació](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)\n",
        "\n",
        "#### Device\n",
        "\n",
        "Si heu emprat _GPU_ per accelerar el procés d'entrenament, els resultats que obtenim de la xarxa també seràn a la _GPU_. **Pytorch** proporciona la funció `cpu()` que retorna una còpia d'aquest objecte a la memòria de la CPU.\n",
        "\n",
        "#### Detach\n",
        "Per poder operar amb els resultats de la predicció emprarem la funció `detach` que retorna un nou Tensor \"separat\" del graf (xarxa) en curs.\n",
        "\n",
        "Per tant per transformar el tensor que retorna la xarxa en un array de la lliberia _Numpy_ caldria fer el següent:\n",
        "\n",
        "  ```\n",
        "  resultat_np = resultat.detach().numpy()\n",
        "  ```\n",
        "Si a més hem executat l'entrenament en _GPU_:\n",
        "  ```\n",
        "  resultat_np = resultat.cpu().detach().numpy()\n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYKUppOc_4JE"
      },
      "outputs": [],
      "source": [
        "def generador(loader):\n",
        "  for data, target in test_loader:\n",
        "    yield data, target\n",
        "\n",
        "\n",
        "#TODO "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.14 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
